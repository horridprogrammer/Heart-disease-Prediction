{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d895d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.25\n",
      "Specificity: 86.21\n",
      "Sensitivity: 84.38\n",
      "F-Measure: 85.28\n",
      "Execution Time: 0.0188 seconds\n"
     ]
    }
   ],
   "source": [
    "#logistic regression ---- cleaveland dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv('robust\\ClevelandScaledoutput.csv')\n",
    "\n",
    "X = data.drop(columns=['num']) \n",
    "y = data['num']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "103b4e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.05\n",
      "Specificity: 81.58\n",
      "Sensitivity: 85.71\n",
      "F-Measure: 83.60\n",
      "Execution Time: 0.0198 seconds\n"
     ]
    }
   ],
   "source": [
    "#logistic Regression -- Hungarian Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv('robust\\HungarianScaledoutput.csv')\n",
    "\n",
    "X = data.drop(columns=['num']) \n",
    "y = data['num']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bced99eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.89%\n",
      "Specificity: 89.66\n",
      "Sensitivity: 84.38\n",
      "F-Measure: 86.93\n",
      "Execution Time: 0.0220 seconds\n"
     ]
    }
   ],
   "source": [
    "#K nearest neighbor ---- Cleaveland Dataset\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "start_time=time.time()\n",
    "import warnings\n",
    "csv_file = 'robust\\ClevelandScaledoutput.csv'\n",
    "data = pd.read_csv(csv_file, delimiter=',')\n",
    "data['target'] = (data['num'] > 0).astype(int)\n",
    "X = data.drop(columns=['num', 'target']) #changing to binary target value\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy* 100:.2f}%')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa8c728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.36%\n",
      "Specificity: 84.21\n",
      "Sensitivity: 76.19\n",
      "F-Measure: 80.00\n",
      "Execution Time: 0.0309 seconds\n"
     ]
    }
   ],
   "source": [
    "# Knearest Neighbor ---- Hungarian Dataset\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "start_time=time.time()\n",
    "import warnings\n",
    "csv_file = 'robust\\HungarianScaledoutput.csv'\n",
    "data = pd.read_csv(csv_file, delimiter=',')\n",
    "data['target'] = (data['num'] > 0).astype(int)\n",
    "X = data.drop(columns=['num', 'target']) #changing to binary target value\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy* 100:.2f}%')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d278c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.25\n",
      "Specificity: 82.76\n",
      "Sensitivity: 87.50\n",
      "F-Measure: 85.06\n",
      "Execution Time: 0.7899 seconds\n"
     ]
    }
   ],
   "source": [
    "#Guassian Process ---- Cleaveland Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv('robust\\ClevelandScaledoutput.csv')\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "numeric_features = list(range(X.shape[1]))\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "kernel = 1.0 * RBF()\n",
    "classifier = GaussianProcessClassifier(kernel=kernel, random_state=0, n_jobs=-1)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy: {accuracy* 100:.2f}')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c82d83bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.75\n",
      "Specificity: 84.21\n",
      "Sensitivity: 85.71\n",
      "F-Measure: 84.96\n",
      "Execution Time: 0.4587 seconds\n"
     ]
    }
   ],
   "source": [
    "#Guassian Process ---- Hungarian Dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv('robust\\HungarianScaledoutput.csv')\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "numeric_features = list(range(X.shape[1]))\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "kernel = 1.0 * RBF()\n",
    "classifier = GaussianProcessClassifier(kernel=kernel, random_state=0, n_jobs=-1)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy: {accuracy* 100:.2f}')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d798cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.16\n",
      "Specificity: 89.66\n",
      "Sensitivity: 90.62\n",
      "F-Measure: 90.14\n",
      "Execution Time: 0.0274 seconds\n"
     ]
    }
   ],
   "source": [
    "# SVM ---- Cleaveland Dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "csv_file = 'robust\\ClevelandScaledoutput.csv'\n",
    "data = pd.read_csv(csv_file, delimiter=',')\n",
    "data['target'] = (data['num'] > 0).astype(int)\n",
    "X = data.drop(columns=['num', 'target']) #changing to binary target value\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a235097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.05\n",
      "Specificity: 81.58\n",
      "Sensitivity: 85.71\n",
      "F-Measure: 83.60\n",
      "Execution Time: 0.0238 seconds\n"
     ]
    }
   ],
   "source": [
    "# SVM ---- Hungarian Dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "csv_file = 'robust\\HungarianScaledoutput.csv'\n",
    "data = pd.read_csv(csv_file, delimiter=',')\n",
    "data['target'] = (data['num'] > 0).astype(int)\n",
    "X = data.drop(columns=['num', 'target']) #changing to binary target value\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bb53b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.69%\n",
      "Specificity: 79.31\n",
      "Sensitivity: 78.12\n",
      "F-Measure: 78.71\n",
      "Execution Time: 0.0078 seconds\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree ---- Cleveland Dataset \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "csv_file = 'robust\\ClevelandScaledoutput.csv'\n",
    "data = pd.read_csv(csv_file, delimiter=',')\n",
    "X = data.drop(columns=['num'])\n",
    "y = data['num']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d78ad694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.27%\n",
      "Specificity: 73.68\n",
      "Sensitivity: 80.95\n",
      "F-Measure: 77.15\n",
      "Execution Time: 0.0061 seconds\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree ---- Hungarian Dataset \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "csv_file = 'robust\\HungarianScaledoutput.csv'\n",
    "data = pd.read_csv(csv_file, delimiter=',')\n",
    "X = data.drop(columns=['num'])\n",
    "y = data['num']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50ceeeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.61\n",
      "Specificity: 89.66\n",
      "Sensitivity: 78.12\n",
      "F-Measure: 83.49\n",
      "Execution Time: 0.0106 seconds\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes ---- Cleaveland Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "csv_file = 'robust\\ClevelandScaledoutput.csv'\n",
    "data = pd.read_csv(csv_file, delimiter=',')\n",
    "data['target'] = (data['num'] > 0).astype(int)\n",
    "X = data.drop(columns=['num', 'target'])\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "naive_bayes = GaussianNB()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "y_pred = naive_bayes.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c49943a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.71\n",
      "Specificity: 44.74\n",
      "Sensitivity: 95.24\n",
      "F-Measure: 60.88\n",
      "Execution Time: 0.0869 seconds\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes ---- Hungarian Dataset (hyper parameter)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "csv_file = 'robust\\HungarianScaledoutput.csv'\n",
    "data = pd.read_csv(csv_file, delimiter=',')\n",
    "data['target'] = (data['num'] > 0).astype(int)\n",
    "X = data.drop(columns=['num', 'target'])\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "naive_bayes = GaussianNB()\n",
    "\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]  # Example values, you can add more\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(naive_bayes, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_naive_bayes = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_naive_bayes.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100:.2f}')\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "676ecbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.89\n",
      "Specificity: 89.66\n",
      "Sensitivity: 84.38\n",
      "F-Measure: 86.93\n",
      "Execution Time: 0.0302 seconds\n"
     ]
    }
   ],
   "source": [
    "# QDA ---- Cleaveland Dataset \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv('robust\\ClevelandScaledoutput.csv')\n",
    "X = data.drop(columns=['num'])\n",
    "y = data['num']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "qda_classifier = QuadraticDiscriminantAnalysis()\n",
    "qda_classifier.fit(X_train, y_train)\n",
    "y_pred = qda_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a14545e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.36\n",
      "Specificity: 84.21\n",
      "Sensitivity: 76.19\n",
      "F-Measure: 80.00\n",
      "Execution Time: 0.1135 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log\n",
      "  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log\n",
      "  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log\n",
      "  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:903: RuntimeWarning: divide by zero encountered in power\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:903: RuntimeWarning: invalid value encountered in multiply\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:906: RuntimeWarning: divide by zero encountered in log\n",
      "  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "# QDA ---- Hungarian Dataset (Hyper Parameter)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv('robust\\HungarianScaledoutput.csv')\n",
    "X = data.drop(columns=['num'])\n",
    "y = data['num']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "qda_classifier = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "param_grid = {\n",
    "    'reg_param': [0.0, 0.1, 0.2, 0.3, 0.4]  # Example hyperparameter, you can add more\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(qda_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_qda_classifier = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_qda_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}\")\n",
    "print(f'Specificity: {specificity * 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity * 100:.2f}')\n",
    "print(f'F-Measure: {f_measure * 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1ba367e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.97\n",
      "Specificity: 86.21\n",
      "Sensitivity: 78.12\n",
      "F-Measure: 81.97\n",
      "Execution Time: 0.0625 seconds\n"
     ]
    }
   ],
   "source": [
    "# Ada Boost ---- Cleaveland Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv('robust\\ClevelandScaledoutput.csv')\n",
    "X = data.drop(columns=['num'])\n",
    "y = data['num']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1)  # You can customize the base classifier\n",
    "adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=50, random_state=42)\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "y_pred = adaboost_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc40433c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.05\n",
      "Specificity: 84.21\n",
      "Sensitivity: 80.95\n",
      "F-Measure: 82.55\n",
      "Execution Time: 0.0807 seconds\n"
     ]
    }
   ],
   "source": [
    "# Ada Boost ---- Hungarian Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv('robust\\HungarianScaledoutput.csv')\n",
    "X = data.drop(columns=['num'])\n",
    "y = data['num']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1)  # You can customize the base classifier\n",
    "adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=50, random_state=42)\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "y_pred = adaboost_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0e001cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.89\n",
      "Specificity: 89.66\n",
      "Sensitivity: 84.38\n",
      "F-Measure: 86.93\n",
      "Execution Time: 0.1115 seconds\n"
     ]
    }
   ],
   "source": [
    "# Bagging ---- Cleaveland Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv('robust\\ClevelandScaledoutput.csv')\n",
    "X = data.drop(columns=['num'])\n",
    "y = data['num']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce19c57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.36\n",
      "Specificity: 84.21\n",
      "Sensitivity: 76.19\n",
      "F-Measure: 80.00\n",
      "Execution Time: 0.1216 seconds\n"
     ]
    }
   ],
   "source": [
    "# Bagging ---- Hungarian Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv('robust\\HungarianScaledoutput.csv')\n",
    "X = data.drop(columns=['num'])\n",
    "y = data['num']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27b5a34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Accuracy: 90.16\n",
      "Specificity: 93.10\n",
      "Sensitivity: 87.50\n",
      "F-Measure: 90.21\n",
      "Execution Time: 0.0560 seconds\n"
     ]
    }
   ],
   "source": [
    "# Boosting LGBM ---- Cleaveland Dataset\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv('robust\\ClevelandScaledoutput.csv')\n",
    "X = data.drop(columns=['num'])\n",
    "y = data['num']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 31,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "model = lgb.train(params, train_data, 100)\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}\")\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2334d70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Accuracy: 76.27\n",
      "Specificity: 78.95\n",
      "Sensitivity: 71.43\n",
      "F-Measure: 75.00\n",
      "Execution Time: 0.0489 seconds\n"
     ]
    }
   ],
   "source": [
    "# Boosting LGBM ---- Hungarian Dataset\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv('robust\\HungarianScaledoutput.csv')\n",
    "X = data.drop(columns=['num'])\n",
    "y = data['num']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 31,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "model = lgb.train(params, train_data, 100)\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}\")\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6589911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 1s 52ms/step - loss: 0.7051 - accuracy: 0.5496 - val_loss: 0.6278 - val_accuracy: 0.6885\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6603 - accuracy: 0.6488 - val_loss: 0.5930 - val_accuracy: 0.7705\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6215 - accuracy: 0.7066 - val_loss: 0.5628 - val_accuracy: 0.7869\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5851 - accuracy: 0.7521 - val_loss: 0.5340 - val_accuracy: 0.8197\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5544 - accuracy: 0.7851 - val_loss: 0.5071 - val_accuracy: 0.8197\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5071 - accuracy: 0.8197\n",
      "Test accuracy: 81.97%\n",
      "Specificity: 89.66\n",
      "Sensitivity: 75.00\n",
      "F-Measure: 81.68\n",
      "Execution Time: 1.1631 seconds\n"
     ]
    }
   ],
   "source": [
    "# Dense Neural Network ---- Cleaveland Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import layers, models\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv('robust\\ClevelandScaledoutput.csv')\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "numeric_features = list(range(X.shape[1]))  # Assuming all features are numeric\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', StandardScaler(), numeric_features)])\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "preprocessor.fit(X_train)\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "model.fit(X_train_transformed, y_train, epochs=5, batch_size=64, validation_data=(X_test_transformed, y_test))\n",
    "y_pred_prob = model.predict(X_test_transformed)\n",
    "y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "test_loss, test_acc = model.evaluate(X_test_transformed, y_test)\n",
    "print(f'Test accuracy: {test_acc * 100:.2f}%')\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a5b16aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 1s 54ms/step - loss: 0.7182 - accuracy: 0.4298 - val_loss: 0.7086 - val_accuracy: 0.5085\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6570 - accuracy: 0.5872 - val_loss: 0.6565 - val_accuracy: 0.6102\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6124 - accuracy: 0.7149 - val_loss: 0.6121 - val_accuracy: 0.6610\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5743 - accuracy: 0.7830 - val_loss: 0.5749 - val_accuracy: 0.7288\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5427 - accuracy: 0.8170 - val_loss: 0.5440 - val_accuracy: 0.7288\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5440 - accuracy: 0.7288\n",
      "Test accuracy: 72.88%\n",
      "Specificity: 71.05\n",
      "Sensitivity: 76.19\n",
      "F-Measure: 73.53\n",
      "Execution Time: 1.1340 seconds\n"
     ]
    }
   ],
   "source": [
    "# Dense Neural Network ---- Hungarian Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import layers, models\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = pd.read_csv('robust\\HungarianScaledoutput.csv')\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "numeric_features = list(range(X.shape[1]))  # Assuming all features are numeric\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', StandardScaler(), numeric_features)])\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "preprocessor.fit(X_train)\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "model.fit(X_train_transformed, y_train, epochs=5, batch_size=64, validation_data=(X_test_transformed, y_test))\n",
    "y_pred_prob = model.predict(X_test_transformed)\n",
    "y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "test_loss, test_acc = model.evaluate(X_test_transformed, y_test)\n",
    "print(f'Test accuracy: {test_acc * 100:.2f}%')\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(f'Specificity: {specificity* 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity* 100:.2f}')\n",
    "print(f'F-Measure: {f_measure* 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a90c67d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SivaRanjan.s\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25684\\175261558.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscikit_learn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.wrappers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('robust\\ClevelandScaledoutput.csv')\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "numeric_features = list(range(X.shape[1]))  # Assuming all features are numeric\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', StandardScaler(), numeric_features)])\n",
    "\n",
    "# Define a function to create model\n",
    "def create_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create KerasClassifier for GridSearchCV\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define parameters grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [5, 10, 15]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(preprocessor.fit_transform(X_train), y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_result.best_estimator_\n",
    "\n",
    "# Evaluate the best model on test data\n",
    "test_accuracy = best_model.score(preprocessor.transform(X_test), y_test)\n",
    "print(f'Test accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "# Predictions\n",
    "y_pred_prob = best_model.predict(preprocessor.transform(X_test))\n",
    "y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "\n",
    "# Execution time\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "# Print results\n",
    "print(f'Specificity: {specificity * 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity * 100:.2f}')\n",
    "print(f'F-Measure: {f_measure * 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ed9a7e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras' has no attribute 'wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25684\\3062645644.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# Create KerasClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscikit_learn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m# Grid search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.keras' has no attribute 'wrappers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('robust\\ClevelandScaledoutput.csv')\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "numeric_features = list(range(X.shape[1]))  # Assuming all features are numeric\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', StandardScaler(), numeric_features)])\n",
    "\n",
    "# Define a function to create model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Grid search parameters\n",
    "param_grid = {\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [5, 10, 15]\n",
    "}\n",
    "\n",
    "# Create KerasClassifier\n",
    "model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(preprocessor.fit_transform(X_train), y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_result.best_estimator_\n",
    "\n",
    "# Evaluate the best model on test data\n",
    "test_accuracy = best_model.score(preprocessor.transform(X_test), y_test)\n",
    "print(f'Test accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "# Predictions\n",
    "y_pred_prob = best_model.predict(preprocessor.transform(X_test))\n",
    "y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "f_measure = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "\n",
    "# Execution time\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "# Print results\n",
    "print(f'Specificity: {specificity * 100:.2f}')\n",
    "print(f'Sensitivity: {sensitivity * 100:.2f}')\n",
    "print(f'F-Measure: {f_measure * 100:.2f}')\n",
    "print(f'Execution Time: {execution_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c49d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
